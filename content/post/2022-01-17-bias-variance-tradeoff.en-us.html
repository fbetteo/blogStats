---
title: Bias Variance Tradeoff
author: ''
date: '2022-01-17'
slug: bias-variance-tradeoff.en-us
categories:
  - estadistica
  - matematica
tags:
  - estadistica
  - Machine Learning
thumbnailImage: https://lh3.googleusercontent.com/Jn2i1YphKhAbS_1w3KSotp7L0BZA3GguSSAEUCCyH9V4g2PtunCuoE0GlY-PkdrsLERb08KiSsNvIMPqpQ=w260-h173-rw
thumbnailImagePosition: left
summary: Some details about Bias Variance Tradeoff
---



<p>Mean squared error (MSE) is a measure of how far our prediction is from the true values of the dependent variable. It’s the expectation of the squared error.</p>
<p>The squared error being:<br />
<span class="math display">\[(Y - \hat \mu(x))^2\]</span>
where Y is the true value and $ (x)$ is the prediction for a given x.</p>
<p>We can decompose it into:<br />
<span class="math display">\[
(Y - \hat \mu(x))^2 \\
= (Y - \mu(x) + \mu(x) - \hat \mu(x)^2) \\
= (Y - \mu(x))^2 + 2(Y - \mu(x))(\mu(x) - \hat \mu(x)) + (\mu(x) - \hat \mu(x))^2
\]</span></p>
<p>So, that’s the squared error. The MSE is the expectation of that.</p>
<p>The expectation is a linear operator so we can apply it independently to different terms of a summation.<br />
The expectation of the first term is the variance of the error intrinsic to the DGP.<br />
The second term goes to 0 because involves <span class="math inline">\(E(Y-\mu(x))\)</span> that is the expectation of the error and that’s equal to 0.<br />
The third term reamins as it is since doesn’t involve random variables.</p>
<p><span class="math display">\[MSE(\hat \mu(x)) = \sigma^2_x + (\mu(x) - \hat \mu(x))^2\]</span></p>
<p>This is our first bias-variance decomposition. The first term is the intrinsic difficulty of the problem to model, is the variance of the error and can not be reduced, it is what it is.<br />
The second term is how off our predictions are regarding the true expected value for that particular X.</p>
<p>This would be fine if we wouldn’t need to consider <span class="math inline">\(\hat \mu(x)\)</span> a random variable itself, since it is dependent on the specific dataset we are using. Given another dataset our estimation would be different despite using the same model methodology.<br />
What we actually want is the MSE of the method used <span class="math inline">\(\hat M\)</span> and not only the result of a particular realization.</p>
<p><span class="math display">\[MSE(\hat M_n(x)) = E[(Y - \hat M_n(X))^2 | X=x] \\
= ... \\
= \sigma^2_x + (\mu(x) -  E[\hat M_n(x)])^2 - V[\hat M_n(x)]
\]</span>
This is our 2nd bias-variance decomposition.<br />
The first term is still the irreducible error.<br />
The second term is the bias of using <span class="math inline">\(\hat M_n\)</span> to approximate <span class="math inline">\(\mu(x)\)</span>. Is the approximation bias/error.<br />
The third term is the variance of the estimate of the regression function. If our estimates have high variance we can have large errors despite using an unbiased approximation.</p>
<p>Flexible methods will be able to approximate <span class="math inline">\(\mu(x)\)</span> closely, however usually using more flexible methods involve increasing the variance of the estimate. That’s the <strong>bias-variance tradeoff</strong>. We need to evaluate how to balance that, sometimes including some bias reduce much more the error by decreasing the variance.<br />
Usually larger N decreases the MSE since it decreases bias and variance error.</p>
<div id="reference" class="section level4">
<h4>Reference</h4>
<p>Based on 1.4.1 from Advanced data analysis from a elementary point of view.</p>
</div>
